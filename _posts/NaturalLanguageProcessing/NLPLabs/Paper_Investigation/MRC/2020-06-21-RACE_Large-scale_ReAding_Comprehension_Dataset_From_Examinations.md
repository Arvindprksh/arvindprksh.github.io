---
layout: post
title: RACE Large-scale ReAding Comprehension Dataset From Examinations
subtitle: Title of paper - RACE Large-scale ReAding Comprehension Dataset From Examinations
category: NLP papers - Machine Reading Comprehension
tags: [neural network, machine reading comprehension]
permalink: /2020/06/21/RACE_Large-scale_ReAding_Comprehension_Dataset_From_Examinations/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This is a brief summary of paper for me to study and organize it, [RACE: Large-scale ReAding Comprehension Dataset From Examinations. Lai et al. EMNLP 2017](https://www.aclweb.org/anthology/D17-1082/) I read and studied. 
{% include MathJax.html %}

This paper proposed the new dataset for reading comprehension that they called RACE.

An example of RACE is as follows:

![Lai et al. EMNLP 2017](/img/Image/NaturalLanguageProcessing/NLPLabs/Paper_Investigation/MRC/2020-06-21-RACE_Large-scale_ReAding_Comprehension_Dataset_From_Examinations/RACE_sample.PNG)


They explain strenghts of their dataset:

The advantages of their proposed dataset over existing large datasets in machine reading comprehension can be summarized as follows:

- All questions and candidate options are generated by human experts, which are intentionally designed to test human agent’s ability in reading comprehension. This makes RACE a relatively accurate indicator for reflecting the text comprehension ability of machine learning systems under human judge.

- The questions are substantially more difficult than those in existing datasets, in terms of the large portion of questions involving reasoning. At the meantime, it is also sufficiently large to support the training of deep learning models.

- Unlike existing large-scale datasets, candidate options in RACE are human generated sentences which may not appear in the original passage. This makes the task more challenging and allows a rich type of questions such as passage summarization and attitude analysis.

- Broad coverage in various domains and writing styles: a desirable property for evaluating generic (in contrast to domain/style-specific) comprehension ability of learning models.

Also comparing with the existing datasets for reading comprehension task which are MCTest, Cloze-style datasets, Datasets with Span-based Answers, and Datasets from Examinations.

In other words, RACE can be viewed as a larger and more difficult version of the MCTest dataset.

They prevent High noise, which is inevitable in cloze-style datasets due to their automatic generation process, from generating in new reading comprehnesion dataset.

To the best of their knowledge, RACE is the first large-scale dataset of this type, where questions are created based on exams designed to evaluate human performance in reading comprehension

They subdivide reasoning types of the questions: 

- Word matching: The question exactly matches a span in the article. The answer is self-evident.

- Paraphrasing: The question is entailed or paraphrased by exactly one sentence in the passage. The answer can be extracted within the sentence.

- Single-sentence reasoning: The answer could be inferred from a single sentence of the article by recognizing incomplete information or conceptual overlap.

- Multi-sentence reasoning: The answer must be inferred from synthesizing information distributed across multiple sentences.

- Insufficient/Ambiguous: The question has no answer or the answer

<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>
They present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students’ ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). They hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension.
</div>
    
<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://www.aclweb.org/anthology/D17-1082/">The paper: RACE: Large-scale ReAding Comprehension Dataset From Examinations. Lai et al. EMNLP 2017</a>
</div>

# Reference 

- Paper 
  - [Arxiv version: RACE Large-scale ReAding Comprehension Dataset From Examinations. Lai et al. Arxiv 2017](https://arxiv.org/abs/1704.04683)
  - [EMNLP 2017 version: RACE Large-scale ReAding Comprehension Dataset From Examinations. Lai et al. EMNLP 2017](https://www.aclweb.org/anthology/D17-1082/)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
    


