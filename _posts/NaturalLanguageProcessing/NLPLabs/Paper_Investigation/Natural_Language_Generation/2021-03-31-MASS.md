---
layout: post
title: MASS- Sequence to Sequence Pre-training for Language Generation
subtitle: Title of paper - MASS- Sequence to Sequence Pre-training for Language Generation
category: NLP papers - Language Model
tags: [language model, pre-training]
permalink: /2021/01/21/BART/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This is a brief summary of paper for me to study and organize it, [arXiv Version: MASS: Sequence to Sequence Pre-training for Language Generation (Song et al., arXiv 2019)](https://arxiv.org/abs/1905.02450) that I read and studied. 
{% include MathJax.html %}

Tho following is the material of my paper seminar on MASS which is composed by me.

I hope someone who want to understand what is the MASS and pre-training in natural language processing field

<div id="tutorial-section">

  <div id="tutorial-title">MASS: Sequence to Sequence Pre-training for Language Generation (Song et al., arXiv 2019)</div>

  <ul class="nav nav-pills">
    <li class="active"><a data-toggle="tab" href="#detailed_version">My Paper seminar presentation</a></li>
  </ul>

  <div class="tab-content">
    <div id="detailed_version" class="tab-pane fade in active">
      <iframe width="560" height="315" src="//www.slideshare.net/slideshow/embed_code/key/xbmEQauxAShmp1"  frameborder="0" allowfullscreen></iframe> 
    </div>
  </div>
</div>
 

For detailed experiment analysis, you can found in [MASS: Sequence to Sequence Pre-training for Language Generation (Song et al., arXiv 2019)](https://arxiv.org/abs/1905.02450)
 
  
<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>
Pre-training and fine-tuning, e.g., BERT (Devlin et al., 2018), have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, they propose MAsked Sequence to Sequence pre-training (MASS) for encoder-decoder based language generation. MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over baselines without pre-training or with other pretraining methods. 
</div>
    
<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://www.aclweb.org/anthology/2020.acl-main.703/">The paper:  BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (Lewis et al., ACL 2020)</a>
</div>

# Reference 

- Paper 
  - [arXiv Version: MASS: Sequence to Sequence Pre-training for Language Generation (Song et al., arXiv 2019)](https://arxiv.org/abs/1905.02450)
 
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
    
- For your information 
   - [BART Presentation Video in ACL 2020](https://slideslive.com/38929218/bart-denoising-sequencetosequence-pretraining-for-natural-language-generation-translation-and-comprehension)
   - [Understand how the XLNet outperforms BERT in Language Modelling](https://medium.com/saarthi-ai/xlnet-the-permutation-language-model-b30f5b4e3c1e)
   - [The Illustrated Transformer in Jay Alammar blog](https://jalammar.github.io/illustrated-transformer/)
   - [Layer Normalization in paperswithcode](https://paperswithcode.com/method/layer-normalization)
   - [What is XLNet and why it outperforms BERT in towards data science by LIANG](https://towardsdatascience.com/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335)
   - [What is Two-Stream Self Attention in XLNet in towards data science by LIANG](https://towardsdatascience.com/what-is-two-stream-self-attention-in-xlnet-ebfe013a0cf3)
