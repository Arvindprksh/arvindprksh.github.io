---
layout: post
title: GLUE- A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.
subtitle: Title of paper - GLUE- A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.
category: NLP papers - Transfer Learning
tags: [neural network, text classification]
permalink: /2019/12/03/GLUE_A_Multi-task_Benchmark_and_analysis_Platform_for_Natural_Language_Understanding/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This is a brief summary of paper for me to study and organize it, [GLUE- A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. Wang et al. arXiv 2019](https://arxiv.org/abs/1804.07461) I read and studied. 
{% include MathJax.html %}

This paper show a platform for Natural Language understaing tasks as follows:

![GLUE Benchmark](/img/Image/NaturalLanguageProcessing/NLPLabs/Paper_Investigation/Text_Classification/2019-12-03-GLUE_A_Multi-task_Benchmark_and_analysis_Platform_for_Natural_Language_Understanding/GLUE_site.PNG)


If you want to submission the platform to test your model across a variety of tasks for NLU.

Visit [GLUE benchmark](https://gluebenchmark.com/) site.

They provide each 9 bechmark sets about NLU tasks. 

To recap, the set is composed of the following: 

![Wang et al. 2019](/img/Image/NaturalLanguageProcessing/NLPLabs/Paper_Investigation/Text_Classification/2019-12-03-GLUE_A_Multi-task_Benchmark_and_analysis_Platform_for_Natural_Language_Understanding/GLUE_data.PNG)

<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>
For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, they introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. They evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.
</div>
    
<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://arxiv.org/abs/1804.07461">The paper: GLUE- A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</a>
</div>

# Reference 

- Paper 
  - [Arxiv version: GLUE- A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. Wang et al. arXiv 2019](https://arxiv.org/abs/1804.07461)
  - [EMNLP 2018 version: GLUE- A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. Wang et al. EMNLP 2018](https://www.aclweb.org/anthology/W18-5446/)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
    
- For your information
  - [GLUE benchmark](https://gluebenchmark.com/)
  
































