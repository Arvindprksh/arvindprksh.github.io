---
layout: post
title: Generative Adversarial Nets
subtitle: Title of paper - Generative Adversarial Nets
category: neural network papers
tags: [neural_network, generative_model]
permalink: /2018/12/27/GAN/
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
---
{% include MathJax.html %}

This article is just brief summary of [Generative Adversarial Networks, an J. Goodfellow et al.(2014)](https://arxiv.org/abs/1406.2661v1) and posting for me to study what is the GAN(Generative Adversarial Networks). 

First of all, This GAN(Genrative Adversarial Nets) is a framework which explains adversarial training for recover traing data distribution, In this paepr, The generator G implicitly defines **a probability distribution \\( p_{g} \\)** as the distribution of the samples \\( G(z) \\) when \\( z \sim p_{z} \\).

Let's see the objective funtion of the GAN.


\\( \min_{G} \max_{D} V(G,D) = \mathbb{E} {x \sim p_{data}(x)} [logD(x)] + \mathbb{E} {z \sim p_{z}(z)}[log(1-D(G(z))] \\)

As you can see above, G and D play two-player minmax game with value function V(G,D). 

More specifically, 

 - Generator network : try to fool the discriminative by generating real-looking data like image and text and so on. 
 - Discriminative network : try to  distinguish between real and fake data like image and text and so on. 
 
traing G and D to their goal, G generates fake data which is similar more and more to real data, whereas D discriminate between fake and real data. 

So The G and D complementarily help improve the performance of each other. 

 - Discrimanator wants to maximize objective such that D(x) is close to 1(real) and D(G(z)) is close to 0(fake)
 
 - Generator wants to minimize objective such that D(G(z)) is close to 1 for discriminator to be fooled into thinking generated G(z) is real.
 
![CS231n Slide](/img/Image/NaturalLanguageProcessing/NLPLabs/Paper_Investigation/Neural_Network/2018-12-27-GAN/GAN.png)

In this paper, both of G and D is multiplayer perceptrons and the output of function V(G,D) is a scalar where measures whether the sample is from traning data or from sample generator create.

There is the following problem where you use the funtion V(G,D) as it is. 

![CS231n Slide](/img/Image/NaturalLanguageProcessing/NLPLabs/Paper_Investigation/Neural_Network/2018-12-27-GAN/G_loss.png)

As you can see above, the original equation may not provide the sufficient gradient for G to learn well as it is in the figure above. So in the paper, they suggest other eqaution for G like this:

> Early in learing, \\( log(1-D_{\theta_{d}}(G_{\theta_{g}}(z))  \\) saturates, that's why the original equation is hard for G to learn well.  
> In the paper, they used a trick. it maximzes log(D(G(z))) rather than minimize log(1-D(G(z))) like the upper curve in the figure above.

Let's see some figure in GAN paper about explaining how for GAN to be trained. 

![GAN paper](/img/Image/NaturalLanguageProcessing/NLPLabs/Paper_Investigation/Neural_Network/2018-12-27-GAN/How_to_be_trained.png)

As you can see above, each line means like this :

 - D, blue, dashed line :  updating discirminative distribution
 
 - back, dotted line \\(  p_{x}  \\) : data generating distribution 
 
 - green, solid line \\(  p_{g}(G)  \\) : genrative distribution
 
 - The horizontal line above is part of the domain of x
 
 - The horizontal line below is the domain from which z is sampled. 
 
 - The upward arrows show how the mapping x = G(z) imposes the non-uniform distribution \\( p_{g} \\) on the transformed samples.
 
 
(a), (b), (c) and (d) in turn update the adversarial traing. (a) currently discriminative doesn't be trained. (b) the discriminator is trained so the blue dotted line is chaneg.

(c) After uan update of G (d) After several steps of tranining, it is ideal step, where \\( p_{data} = p_{g} \\). The discriminator is not able to distinguish between the two distribution. i.e. D(x) =  1/2.

The algorithm of adversarial trainig is the following : 

![GAN paper](/img/Image/NaturalLanguageProcessing/NLPLabs/Paper_Investigation/Neural_Network/2018-12-27-GAN/Adversarial_training.png)


The following is just mathematical analysis of GAN peper equation. if you want to know more, read But if not, skipping is good.


From now on, go through theoretical results they argue about GAN 

The theoretical results is based on a non parametric setting.

> Proposition 1. For G fixed, the optimal discriminator D is \\( D^{\*} {G}(x) = p_{data}(x) / (p_{data}(x) + p_{g}(x)) \\)

**proof.** The trainig criterion for the discriminator D, given any generator G, is to maximize the quantity V(G, D)

\\( V(G, D) =  \int_{x} p_{data}(x)log(D(x))dx + \int_{z} p_{z}(z)log(1-D(g(z)))dz \\)
\\( V(G, D) =  \int_{x} p_{data}(x)log(D(x))dx + p_{g}(x)log(1 - D(x))dx \\)

In the eqaution, Expected value of latent value z and data x change into data space x by using generator that estimate the data distribution with latent variable z.

V(G, D) is simply expressed like this:

For any (a,b) \\( \in \mathbb{R} ^{2} )\\, the function y -> a\*log(y) + b\*log(1-y) achieves maximum value in [0,1] at a/(a+b).

if you get derivative of the function y -> a\*log(y) +  b\*log(1-y) with respect to y for \\( argmax_{y} f(y) \\), suppose the base of logarithms above is e.

y' = a/y - b/(1-y) and when y' = 0, the value of y is a point for maxmimum value of y.

> 0 = a/y - b/(1-y)  
> 0 = a*(1-y) - b*y   
> (a+b)*y  =  a    
> y = a/(a+b)  

the funtion y -> a\*log(y) + b\*log(1-y) reaches maximum in \[0,1\] at a/(a+b).


\\( \min_{G} \max_{D} V(G,D) = \mathbb{E} {x \sim p_{data}(x)} [logD(x)] + \mathbb{E} {z \sim p_{z}(z)} [log(1-D(G(z))] \\) is reformulated as :

\\( C(G) = \max_{D} V(G,D) \\)
     \\( = \mathbb{E} {x \sim p_{data}} [log(D^\*_{G}(x))] + \mathbb{E} {z \sim p_{z}} [log(1-D^\*_{G}(G(z)))]  \\)
     \\( = \mathbb{E} {x \sim p_{data}} [log(D^\*_{G}(x))] + \mathbb{E} {x \sim p_{g}} [log(1-D^\*_{G}(x))]  \\)
     \\( = \mathbb{E} {x \sim p_{data}} [log(p_{data}(x) / (p_{data}(x) + p_{g}(x))] + \mathbb{E} {x \sim p_{g}} [log(p_{g}(x) / (p_{data}(x) + p_{g}(x))])  \\)


##### Therorem 1. The global minimum of the virtual training criterion C(G) is achivied if and only if \\( p_{g} = p_{data} \\). At the point, C(G) reaches the value -log4.


**proof.** for \\( p_{g} = p_{data}, p_{data}(x) / (p_{data}(x) + p_{g}(x)) =  p_{g}(x) / (p_{data}(x) + p_{g}(x)) = 1/2 \\), C(G) is equal to : 

 \\( \mathbb{E} {x \sim p_{data}} [-log2] + \mathbb{E} {x \sim p_{g}} [-log2] = -log4 \\)
 
 and then if you subtract -log4 from \\( C(G) = V(D^*_{G}, G) \\), we obatain : 
 
\\( C(G) = -log4 + log4 + \mathbb{E} {x \sim p_{data}}[log(p_{data}(x) / p_{data}(x) + p_{g}(x))] + \mathbb{E} {x \sim p_{data}}[log(p_{data}(x) / p_{data}(x) + p_{g}(x))] \
         = -log4 + \mathbb{E} {x \sim p_{data}}[log(p_{data}(x) / p_{data}(x) + p_{g}(x))] + log2 + \mathbb{E} {x \sim p_{data}}[log(p_{data}(x) / p_{data}(x) + p_{g}(x))] + log2 \
         = -log4 + KL(p_{data} \|\| (p_{data}+p_{g})/2)) + KL(p_{g} \|\| (p_{data}+p_{g})/2))  \
    and then, JSD(P \|\| Q) = 1/2*D_{KL}(P\|\|M) + 1/2*D_{KL}(Q\|\|M) where M = 1/2*(P+Q) \\)
 
 so, \\( C(G) = -log4 + 2*JSD(p_{data} \|\| p_{g}) \\). 

 
Since Jensen-Shanon divergence between two distribution \\( JSD(p_{data} \|\| p_{g}) \\) is alway non-negative and zero only when they are eqaul.

They showed that C*=-log4 is global minimum of C(G) and the only solution is \\( p_{g} = p_{data}. \\)


<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>
In this paper, they provide a framework for estimating generative models via adversarial process in which they simultaneously train two models: a generative model G that capture the data distribution and a disciminative model D that estimates the probability(a scalar) that a sample came from the trainig data rather than G. The trainig method is the minmax two player game. G and D can become any funtion to the goal which is that G would recover the training data distribution and D would converges to 1/2 everywhere.
</div>
  
  
<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://arxiv.org/abs/1406.2661">The paper: Generatvie Adversarial Networks</a>
</div>

# Reference 

- Paper 
  - [Generative Adversarial Nets on archive](https://arxiv.org/abs/1406.2661)
  - [Generatvie Adversarial Nets on NIPS](https://papers.nips.cc/paper/5423-generative-adversarial-nets)
 
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
  
- For your information
  - [Image Completion with DeepLearning in Tensorflow](http://bamos.github.io/2016/08/09/deep-completion/#step-1-interpreting-images-as-samples-from-a-probability-distribution)
  - [A Beginner's Guide to Generative Adversarial Networks(GANs)](https://skymind.ai/wiki/generative-adversarial-network-gan)
  - [Quora of Yann LeCun](https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning/answer/Yann-LeCun?srid=nZuy)
  - [Quora](https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning)
  - [GAN Lab](https://poloclub.github.io/ganlab/)
  - [CS231N slide lecture13 2018](http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture12.pdf)
  - [GAN github1-hwalsuklee/tensorflow-generative-model-collections](https://github.com/hwalsuklee/tensorflow-generative-model-collections/blob/master/GAN.py)
  - [GAN github2-JJashim/GANs-Tensorflow](https://github.com/JJashim/GANs-Tensorflow/blob/master/GANs-TensorFlow.ipynb)
  - [GAN github3-wiseodd/generative-models](https://github.com/wiseodd/generative-models)
  - [GAN github4-revsic/tf-vanilla-gan](https://github.com/revsic/tf-vanilla-gan/blob/master/model.py)
  - [GAN github5-goodfeli/adversarial](https://github.com/goodfeli/adversarial)

  - [Ian Goodfellow et al.’s Deep Learning Book](http://www.deeplearningbook.org/)
  - [Image Kernel Explained Visually](http://setosa.io/ev/image-kernels/)
  - [Expected value on wikipedia](https://en.wikipedia.org/wiki/Expected_value)
  - [Marginal Probability on wikipedia](https://en.wikipedia.org/wiki/Marginal_distribution)
  - [Kullback-Leibler divergence on wikipedia](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)


- Korean Ver.
  - [GAN Tutorial](https://kakalabblog.wordpress.com/2017/07/27/gan-tutorial-2016/)
  - [Generative Adversarial Nets by Jaejun Yoo's Playground](http://jaejunyoo.blogspot.com/2017/01/generative-adversarial-nets-1.html)
  - [Information and KL Divergence](https://ratsgo.github.io/statistics/2017/09/22/information/)

































