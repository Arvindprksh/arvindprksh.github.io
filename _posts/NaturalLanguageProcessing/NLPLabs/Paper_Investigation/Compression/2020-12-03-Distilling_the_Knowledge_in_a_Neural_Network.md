---
layout: post
title: Distilling the Knowledge in a Neural Network
subtitle: Title of paper - Distilling the Knowledge in a Neural Network
category: NLP papers - Compression
tags: [Compression]
permalink: /2020/12/03/Distilling_the_Knowledge_in_a_Neural_Network/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This is a brief summary of paper for me to study and organize it, [Distilling the Knowledge in a Neural Network (Luong et al., NIPS Deep Learning and Representation Workshop 2015)](https://research.google/pubs/pub44873/) that I read and studied. 
{% include MathJax.html %}

They are saying that there is conflicting contraints between training and deployment. 

For example, in large-scale machine learning, many similar models are usually uesd on training stage and deployment stage in spite of the different requirements. 

In most cases, many models are needed to extract useful structure from dataset to handle the task but in deployment stage, it it important latency and computation resource than extracting useful information. 

So they proposed that once the cumbersome model has been trained, the knowledge of the cumbersome model is transferrend to a small model.

They called it "distilation", in their paper, an obvious way for transferring the generalization ability of the cumbersome model to a small model is to use the class probabilities produced by the cumbersome model as “soft targets” for training the small model.

When transferring the knowlege of cumbersome model with tranfer set, transfer set is whether it is ogriginal traning set or other different set uesed for only transferring the knowledge with soft targets.

They said the softer softmax output is more imformative than harder softmax output(for detailed information, see the vedio below).

$$Softsoftmax(X)=\frac{exp(z_i/T)}{\sum_{j} exp(z_j/T)}$$

As you can see soft softmax above, when T=1, it get the standard softmax function. As T grows, the probability distribution generated by softmax function becomes softer providing more information as to which classes the model found more similar to the predicted class. This is called “**dark knowledge**” embedded in the model.

When they trained a small model with transfer set, they used the same temperature T.

if the correct labels are known for all or some of the transfer set, they imporoved their method by also training the distilled model to produce the correct labels. 

There are simply two ways to improve their method. 
one is to use the correct labels to modify the soft targets and the other is to simply use a weighted average of two different objective functions.

In their experiment they used the latter as follow:

>>The first objective function is the cross entropy with the soft targets and this cross entropy is computed using the same high temperature in the softmax of the distilled model as was used for generating the soft targets from the cumbersome model.   
>>The second objective function is the cross entropy with the correct labels which is computed using exactly the same logits in softmax of the distilled model but at a temperature of 1.  

other than "**Knowledge distillation**", they propose a new type ensemble composed of one or more generalist models and many speciallist models. 

For detailed experiment analysis, you can found in [Distilling the Knowledge in a Neural Network (Luong et al., NIPS Deep Learning and Representation Workshop 2015)](https://research.google/pubs/pub44873/)


<div id="tutorial-section">

  <div id="tutorial-title">TTIC Distinguished Lecture Series - Geoffrey Hinton</div>

  <ul class="nav nav-pills">
    <li class="active"><a data-toggle="tab" href="#dark_knowledge">Dark Knowledge</a></li>
    <li><a data-toggle="tab" href="#dark_knowledge_slide">Dark Knowledge Slides</a></li>
  </ul>

  <div class="tab-content">
    <div id="dark_knowledge" class="tab-pane fade in active">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/EK61htlw8hY" width="560" height="315" frameborder="0" allowfullscreen></iframe>
    </div>
    <div id="dark_knowledge_slide" class="tab-pane fade">
      <iframe width="560" height="315" src="https://www.ttic.edu/dl/dark14.pdf" width="560" height="315" frameborder="0" allowfullscreen></iframe>
    </div>
  </div>
</div>
    
<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>
A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. The previous research have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and they develop this approach further using a different compression technique. They also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.
</div>
    
<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://research.google/pubs/pub44873/">The paper: Distilling the Knowledge in a Neural Network (Luong et al., NIPS Deep Learning and Representation Workshop 2015</a>
</div>

# Reference 

- Paper 
  - [arXiv Version: Distilling the Knowledge in a Neural Network (Hinton et al., arXiv 2015)](https://arxiv.org/abs/1503.02531v1)
  - [NIPS Version: Distilling the Knowledge in a Neural Network (Hinton et al., NIPS Deep Learning and Representation Workshop 2015)](https://research.google/pubs/pub44873/)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
   
- For information 
  - [English Version: Dark Knowlege of Neural Netowrks](https://medium.com/analytics-vidhya/knowledge-distillation-dark-knowledge-of-neural-network-9c1dfb418e6a)
  - [Korean Version: Lunit Tech Blog](https://blog.lunit.io/2018/03/22/distilling-the-knowledge-in-a-neural-network-nips-2014-workshop/)
  - [TTIC Distinguished Lecture Series-Geoffrey Hinton slides](https://www.ttic.edu/dl/dark14.pdf)

