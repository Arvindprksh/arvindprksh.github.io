---
layout: post
title: RoBERTa- Cross-lingualLanguageModelPretraining
subtitle: Title of paper - Cross-lingualLanguageModelPretraining
category: NLP papers - Language Model
tags: [language model, translation]
permalink: /2021/05/07/Cross-lingual_Language_Model_Pretraining/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This is a brief summary of paper for me to study and organize it, [Cross-lingual Language model Pretraining (Conneau and Lample NIPS 2019)](https://proceedings.neurips.cc/paper/2019/hash/c04c19c2c2474dbf5f7ac4372c5b9af1-Abstract.html) that I read and studied. 
{% include MathJax.html %}

<div id="tutorial-section">

  <div id="tutorial-title">Cross-lingual Language model Pretraining (Conneau and Lample NIPS 2019)</div>

  <ul class="nav nav-pills">
    <li class="active"><a data-toggle="tab" href="#detailed_version">My presentation</a></li>
  </ul>

  <div class="tab-content">
    <div id="detailed_version" class="tab-pane fade in active">
      <iframe width="560" height="315" src="//www.slideshare.net/slideshow/embed_code/key/DMYywz04vX0psq"  frameborder="0" allowfullscreen></iframe> 
    </div>
  </div>
</div>
 
 
For detailed experiment analysis, you can found in [Cross-lingual Language model Pretraining (Conneau and Lample NIPS 2019)](https://proceedings.neurips.cc/paper/2019/hash/c04c19c2c2474dbf5f7ac4372c5b9af1-Abstract.html)
  
<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>
Recent studies have demonstrated the efﬁciency of generative pretraining for English natural language understanding. In this work, they extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. They propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language modelo bjective. They obtain state-ofthe-art resultson cross-lingual classiﬁcation, unsupervisedand supervised machine translation. On XNLI, their approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, they obtain 34.3 BLEU on WMT’16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, they obtain a new state of the art of 38.5 BLEU on WMT’16 Romanian-English, outperforming the previous best approach by more than 4 BLEU.
</div>
    
<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://proceedings.neurips.cc/paper/2019/hash/c04c19c2c2474dbf5f7ac4372c5b9af1-Abstract.html">The paper: Cross-lingual Language model Pretraining (Conneau and Lample NIPS 2019)</a>
</div>

# Reference 

- Paper 
  - [arXiv Version: Cross-lingual Language model Pretraining (Lample and Conneau, arXiv 2019)](https://arxiv.org/abs/1901.07291v1)
  - [NIPS 2019 : Cross-lingual Language model Pretraining (Conneau and Lample NIPS 2019)](https://proceedings.neurips.cc/paper/2019/hash/c04c19c2c2474dbf5f7ac4372c5b9af1-Abstract.html)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
 
- For you information
  - [Cross-lingual Language model Pretraining slide (Conneau and Lample NIPS 2019)](https://nips.cc/media/Slides/nips/2019/westexhibitionhalla(12-15-50)-12-16-40-15839-cross-lingual_l.pdf)


