---
layout: post
title: BART- Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
subtitle: Title of paper - BART- Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
category: NLP papers - Language Model
tags: [language model, pre-training]
permalink: /2021/01/21/BART/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This is a brief summary of paper for me to study and organize it, [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (Lewis et al., ACL 2020)](https://www.aclweb.org/anthology/2020.acl-main.703/)
   that I read and studied. 
{% include MathJax.html %}


 
 
 
 
 
For detailed experiment analysis, you can found in [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (Lewis et al., ACL 2020)](https://www.aclweb.org/anthology/2020.acl-main.703/)
  
<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>

</div>
    
<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://www.aclweb.org/anthology/2020.acl-main.703/">The paper:  BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (Lewis et al., ACL 2020)</a>
</div>

# Reference 

- Paper 
  - [arXiv Version: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (Lewis et al., arXiv 2020)](https://arxiv.org/abs/1910.13461)
  - [ACL Version: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (Lewis et al., ACL 2020)](https://www.aclweb.org/anthology/2020.acl-main.703/)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
    
- For your information 
   - [BART Presentation Video in ACL 2020](https://slideslive.com/38929218/bart-denoising-sequencetosequence-pretraining-for-natural-language-generation-translation-and-comprehension)
   - [Understand how the XLNet outperforms BERT in Language Modelling](https://medium.com/saarthi-ai/xlnet-the-permutation-language-model-b30f5b4e3c1e)
   - [The Illustrated Transformer in Jay Alammar blog](https://jalammar.github.io/illustrated-transformer/)
   - [Layer Normalization in paperswithcode](https://paperswithcode.com/method/layer-normalization)
   - [What is XLNet and why it outperforms BERT in towards data science by LIANG](https://towardsdatascience.com/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335)
   - [What is Two-Stream Self Attention in XLNet in towards data science by LIANG](https://towardsdatascience.com/what-is-two-stream-self-attention-in-xlnet-ebfe013a0cf3)
