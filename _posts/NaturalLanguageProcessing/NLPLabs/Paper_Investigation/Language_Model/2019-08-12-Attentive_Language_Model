---
layout: post
title: Attentive Language models
subtitle: Title of paper - Attentive Language Models
category: NLP papers - LM
tags: [neural_network, language model]
permalink: /2019/08/12/Attentive_Language_Model/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This is a brief summary of paper, [Attentive Language Models, Salton et al.(2017)](https://www.aclweb.org/anthology/papers/I/I17/I17-1045/). 

The reason I summarize the paper is to study attention mechanism and arrange the paper I read.

{% include MathJax.html %}

The concept of this paper is attention mechanism in a memory buffer on language model of NLP task. 

The recurrent neural network has improved the state-of-the-art in language model.

There is problem as follows

> RNN-LMs sequentially propagate forward a context vector by integrating the information generated by each prediction step into the context used for the next prediction.
> Oen consequence of this forward propagation of information is that older information tend






<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>
In this paper, we extend Recurrent Neural Network Language Models (RNN-LMs) with an attention mechanism. We show that an “attentive” RNN-LM (with 11M parameters) achieves a better perplexity than larger RNN-LMs (with 66M parameters) and achieves performance comparable to an ensemble of 10 similar sized RNN-LMs. We also show that an “attentive” RNN-LM needs less contextual information to achieve similar results to the state-of-the-art on the wikitext2 dataset.
</div>
    
<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://www.aclweb.org/anthology/papers/I/I17/I17-1045/">The paper: Attentive Language Models</a>
</div>

# Reference 

- Paper 
  - [Attentive Language Models](https://www.aclweb.org/anthology/papers/I/I17/I17-1045/)
  
 
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
  
- How to use MathJax
  - [Stackexchange1](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)
  - [Stackexchange2](https://tex.stackexchange.com/questions/5223/command-for-argmin-or-argmax)
  
- For your information
  - [Attention? Attention! on Lil'Log blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
  - [Self-Attention Mechanisms in Natural Language Prcessing on Alibabacloud](https://www.alibabacloud.com/blog/self-attention-mechanisms-in-natural-language-processing_593968)
  - [Deep Learning for NLP Best Practices on ruder blog](http://ruder.io/deep-learning-nlp-best-practices/index.html#fn2)
  - [Soft & Hard Attention on Jonathan Hui Blog](https://jhui.github.io/2017/03/15/Soft-and-hard-attention/)
  - [Attention Mechanism on Heuritech](https://blog.heuritech.com/2016/01/20/attention-mechanism/)
  - [A Brief Overview of Attention Mechanism on Medium](https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129)
  - [Attention and Memory in Deep Learning and NLP on WILDML](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)
  - [Attention Mechanism in Neural Network on Hackernoon](https://hackernoon.com/attention-mechanism-in-neural-network-30aaf5e39512)
  
  
  - Kor ver
    - [Neural Turing machine no norman](https://norman3.github.io/papers/docs/neural_turing_machine.html)
  
  - Slide 
    - [Attention is all you need on whikwon slideshare](https://www.slideshare.net/WhiKwon/attention-mechanism)
    - [Attention Mechanisms with Tensorflow on KeonKim slideshare](https://www.slideshare.net/KeonKim/attention-mechanisms-with-tensorflow)

































