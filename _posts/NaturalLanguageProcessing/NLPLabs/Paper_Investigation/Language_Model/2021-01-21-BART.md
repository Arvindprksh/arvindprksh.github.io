---
layout: post
title: BART- Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
subtitle: Title of paper - BART- Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
category: NLP papers - Language Model
tags: [language model, pre-training]
permalink: /2021/01/21/BART/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This is a brief summary of paper for me to study and organize it, [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (Lewis et al., ACL 2020)](https://www.aclweb.org/anthology/2020.acl-main.703/) that I read and studied. 
{% include MathJax.html %}


Tho following is the material of my paper seminar on BART which is composed by me.

It consists of two types of presentation, 1) detailed version presentation and 2) short version presentation. 

The below has video of the author presentation. 

I hope someone who want to understand what is the BART and pre-training in natural language processing field

<div id="tutorial-section">

  <div id="tutorial-title">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (Lewis et al., ACL 2020)</div>

  <ul class="nav nav-pills">
    <li class="active"><a data-toggle="tab" href="#detailed_version">My detailed version presentation</a></li>
    <li><a data-toggle="tab" href="#short_version">My short version presentation</a></li>
    <li><a data-toggle="tab" href="#author_presentation_video"> BART of author presentation</a></li>
  </ul>

  <div class="tab-content">
    <div id="detailed_version" class="tab-pane fade in active">
      <iframe width="560" height="315" src="//www.slideshare.net/slideshow/embed_code/key/TOcOjh8EhDytf"  frameborder="0" allowfullscreen></iframe> 
    </div>
    <div id="short_version" class="tab-pane fade">
      <iframe width="560" height="315" src="//www.slideshare.net/slideshow/embed_code/key/eZSqyLGLn6u658" frameborder="0" allowfullscreen></iframe> 
    </div>
    <div id="author_presentation_video" class="tab-pane fade">
      <iframe width="560" height="315" src="https://slideslive.com/38929218/bart-denoising-sequencetosequence-pretraining-for-natural-language-generation-translation-and-comprehension" frameborder="0" allowfullscreen></iframe>
    </div>
  </div>
</div>
 
 
For detailed experiment analysis, you can found in [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (Lewis et al., ACL 2020)](https://www.aclweb.org/anthology/2020.acl-main.703/)
  
<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>
They present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. They evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. They also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.
</div>
    
<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://www.aclweb.org/anthology/2020.acl-main.703/">The paper:  BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (Lewis et al., ACL 2020)</a>
</div>

# Reference 

- Paper 
  - [arXiv Version: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (Lewis et al., arXiv 2020)](https://arxiv.org/abs/1910.13461)
  - [ACL Version: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (Lewis et al., ACL 2020)](https://www.aclweb.org/anthology/2020.acl-main.703/)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
    
- For your information 
   - [BART Presentation Video in ACL 2020](https://slideslive.com/38929218/bart-denoising-sequencetosequence-pretraining-for-natural-language-generation-translation-and-comprehension)
   - [Understand how the XLNet outperforms BERT in Language Modelling](https://medium.com/saarthi-ai/xlnet-the-permutation-language-model-b30f5b4e3c1e)
   - [The Illustrated Transformer in Jay Alammar blog](https://jalammar.github.io/illustrated-transformer/)
   - [Layer Normalization in paperswithcode](https://paperswithcode.com/method/layer-normalization)
   - [What is XLNet and why it outperforms BERT in towards data science by LIANG](https://towardsdatascience.com/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335)
   - [What is Two-Stream Self Attention in XLNet in towards data science by LIANG](https://towardsdatascience.com/what-is-two-stream-self-attention-in-xlnet-ebfe013a0cf3)
