---
layout: post
title: BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding. Devlin et al. NAACL. 2019.
subtitle: Title of paper - BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding. Devlin et al. NAACL. 2019.
category: NLP papers - Transfer Learning
tags: [neural network, contextual embedding]
permalink: /2019/12/02/BERT_Pre-Training_of_Deep_Bidirectional_Transformers_for_Language_Understanding/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This is a brief summary of paper for me to study and organize it, [BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al. NAACL 2019)](https://www.aclweb.org/anthology/N19-1423/) I read and studied. 
{% include MathJax.html %}


The following is the material for the presenation on paper seminar in my class.


<div id="tutorial-section">

  <div id="tutorial-title">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</div>

  <ul class="nav nav-pills">
    <li class="active"><a data-toggle="tab" href="#refrigerator">Selected Topics in Computer Science class</a></li>
  </ul>

  <div class="tab-content">
    <div id="refrigerator" class="tab-pane fade in active">
      <iframe src="//www.slideshare.net/slideshow/embed_code/key/1sjtqMCMFDNc59" width="560" height="315" frameborder="0" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/HyunYoungLee3/paper-seminarbert-pretraining-of-deep-bidirectional-transformers-for-language-understanding" title="(Paper seminar)BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" target="_blank">(Paper seminar)BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> </strong> from <strong><a href="https://www.slideshare.net/HyunYoungLee3" target="_blank">hyunyoung Lee</a></strong> </div>
    </div>
  </div>
</div>

<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>
They introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, which called ELMo and OPENAI-GPT, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).
</div>
    
<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://www.aclweb.org/anthology/N19-1423/">The paper: BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al. NAACL 2019)</a>
</div>

# Reference 

- Paper 
  - [arXiv version: BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al. arXiv 2019)](https://arxiv.org/abs/1810.04805)
  - [NAACL 2019 version: BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al. NAACL 2019)](https://www.aclweb.org/anthology/N19-1423/)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
    
- For your information
  - [BERT on paperwithcode](https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional)
  - [BERT Explained: State of the are langauge model for NLP on Towards Data Science](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
  - [A review of BERT based models on Towards Data Science](https://towardsdatascience.com/a-review-of-bert-based-models-4ffdc0f15d58)
  - [Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing on Google AI blog](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)
  - [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) on Jay Alammar blog](http://jalammar.github.io/illustrated-bert/)



























