---
layout: post
title: Joint Learning of Character and Word Embeddings
subtitle: Title of paper - Joint Learning of Character and Word Embeddings
category: NLP papers - Character Embedding
tags: [character embedding]
permalink: /2020-05-03-Joint_Learning of_Character_and_Word_Embeddingsg/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This is a brief summary of paper for me to study and arrange for [Joint learning of character and word embeddings. Chen et al. 2015 IJCAI](https://www.ijcai.org/Proceedings/15/Papers/178.pdf) I read and studied. 
{% include MathJax.html %}

This paper is a research ralted to word embedding to handle internal information with character.

The existing model for word embedding considers the external context of words without internal information of a word. 

So they propose joint learning of character and word embeddigs as follows:

![Chen et al. 2015 IJCAI](/img/Image/NaturalLanguageProcessing/NLPLabs/Paper_Investigation/Word2Vec/2020-05-03-Joint_Learning of_Character_and_Word_Embeddings/CWE1.PNG)

They propose various variants of joint learning of character word embeddings named as character-enhanced word embedding model (CWE)

- Position-based character embeddigs for CWE

![Chen et al. 2015 IJCAI](img/Image/NaturalLanguageProcessing/NLPLabs/Paper_Investigation/Word2Vec/2020-05-03-Joint_Learning of_Character_and_Word_Embeddings/CWE2.PNG)

- cluster-based character embedding for CWE 

![Chen et al. 2015 IJCAI](/img/Image/NaturalLanguageProcessing/NLPLabs/Paper_Investigation/Word2Vec/2020-05-03-Joint_Learning of_Character_and_Word_Embeddings/CWE3.PNG)
Most word embedding methods take a word as a basic unit and learn embeddings according to wordsâ€™external contexts, ignoring the internal structures of words. However, in some languages such as Chinese, a word is usually composed of several characters and contains rich internal information. The semantic meaning of a word is also related to the meanings of its composing characters. Hence, they take Chinese for example, and present a characterenhanced word embedding model (CWE). In order to address the issues of character ambiguity and non-compositional words, they propose multiple prototype character embeddings and an effective word selection method. they evaluate the effectiveness of CWE on word relatedness computation and analogical reasoning. The results show that CWE outperforms other baseline methods which ignore internal character information.
<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>

</div>
    
<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://www.ijcai.org/Proceedings/15/Papers/178.pdf">The paper:  Joint learning of character and word embeddings. Chen et al. 2015 IJCAI</a>
</div>

# Reference 

- Paper 
  - [ACM publisher: Joint learning of character and word embeddings. Chen et al. 2015 IJCAI](https://dl.acm.org/doi/10.5555/2832415.2832421)
  - [IJCAI 2015 version: Joint learning of character and word embeddings. Chen et al. 2015 IJCAI](https://www.ijcai.org/Proceedings/15/Papers/178.pdf)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
    

