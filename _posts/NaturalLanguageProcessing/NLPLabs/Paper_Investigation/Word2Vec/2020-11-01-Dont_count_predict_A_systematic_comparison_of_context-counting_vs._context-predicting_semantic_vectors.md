---
layout: post
title: Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors
subtitle: Title of paper - Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors
category: NLP papers - Word Embedding
tags: [word embedding]
permalink: /2020/11/01/Dont_count_predict_A_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This is a brief summary of paper for me to study and organize it, [Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. Baroni et al. ACL 2014](https://www.aclweb.org/anthology/P14-1023/) I read and studied. 
{% include MathJax.html %}

They compare count-based DSM (distributional semantic model) with predictive-based DSM.

In their paper, they overcome the comparison scarity problem by providing a direct evalutation of count and predict DSM s across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks. 

As you can know from the title of their paper, their title already gave away what they discovered.

For the detailed result of comparison, it can be found in [Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. Baroni et al. ACL 2014](https://www.aclweb.org/anthology/P14-1023/)

<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>
Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the literature is still lacking a systematic comparison of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, they perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to their own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts.
</div>
    
<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://www.aclweb.org/anthology/P14-1023/">The paper: Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. Baroni et al. ACL 2014</a>
</div>

# Reference 

- Paper 
  - [ACL 2014 version: Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. Baroni et al. ACL 2014](https://www.aclweb.org/anthology/P14-1023/)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
    


