---
layout: post
title: Inductive Representation Learning on Large Graphs
subtitle: Title of paper - Inductive Representation Learning on Large Graphs
category: NLP papers - Graph Neural Networks
tags: [graph neural networks]
permalink: /2021/01/20/Inductive_Representation_Learning_on_Large_Graphs/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This is a brief summary of paper for me to study and organize it, [Inductive Representation Learning on Large Graphs (Hamilton et al., NIPS 2017)](https://dl.acm.org/doi/10.5555/3294771.3294869) that I read and studied. 
{% include MathJax.html %}

This method is Spatial-based Graph Convolutional Networks.

###For training, they used the same neural network structure such as [Sutskever et al., NIPS 2014](https://arxiv.org/abs/1409.3215) which uses a deep LSTM ot encode the input sequence and a separate deep LSTM to output the translation.

###For detailed experiment analysis, you can found in [Addressing the Rare Word Problem in Neural Machine Translation (Luong et al., ACL and IJCNLP 2015)](https://www.aclweb.org/anthology/P15-1002/)

<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>

</div>
    
<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://dl.acm.org/doi/10.5555/3294771.3294869">The paper: Inductive Representation Learning on Large Graphs (Hamilton et al., NIPS 2017)</a>
</div>

# Reference 

- Paper 
  - [arXiv Version: Inductive Representation Learning on Large Graphs (Hamilton et al., arXiv 2017)](https://arxiv.org/abs/1706.02216)
  - [NIPS Version: Inductive Representation Learning on Large Graphs (Hamilton et al., NIPS 2017)](https://dl.acm.org/doi/10.5555/3294771.3294869)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
    
- For your information
  - Korean Version
    - [Graph Convolutional Network, GCN](https://untitledtblog.tistory.com/152)
  
  - English Version
    - [Understanding Graph Convolutional Networks for Node Classification](https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b)
    - [Getting the Intuition of Graph Neural Networks](https://medium.com/analytics-vidhya/getting-the-intuition-of-graph-neural-networks-a30a2c34280d)
    - [spectral vs spatial convolution on graph neural network](https://ai.stackexchange.com/questions/14003/what-is-the-difference-between-graph-convolution-in-the-spatial-vs-spectral-doma)
    - [How to do Deep Learning on Graphs with Graph Convolutional Networks:part1](https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780)
    - [How to do Deep Learning on Graphs with Graph Convolutional Networks:part2](https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-62acf5b143d0)    
    - [What is Geometric Deep Learning?](https://flawnsontong.medium.com/what-is-geometric-deep-learning-b2adb662d91d)
    - [Everything you need to know about Graph Theory for Deep Learning:part0](https://towardsdatascience.com/graph-theory-and-deep-learning-know-hows-6556b0e9891b)
    - [Graph Embedding for Deep Learning](https://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4)
    - [Graph Convolutional Networks for Geometric Deep Learning:part2](https://towardsdatascience.com/graph-convolutional-networks-for-geometric-deep-learning-1faf17dee008)
